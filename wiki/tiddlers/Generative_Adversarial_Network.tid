created: 20161021080332245
modified: 20181003105915687
tags: [[Deep Generative Models]]
title: Generative Adversarial Network
type: text/vnd.tiddlywiki

! Theory
The GAN framework consists of two players - the //generator// $G_\phi(z)$ whose aim is to create realistic samples and the //discriminator// $D_\theta(x)$ whose aim is to classify an instance as having come from training data or the generator. Their cost functions are defined as:
$$J^{(D)}(\phi, \theta) = -\frac12\mathbb E_{x\sim p_{real}}\log D_\theta(x)-\frac12\mathbb E_z\log(1-D_\theta(G_\phi(z)))$$
$$J^{(G)}(\phi, \theta) = \frac12\mathbb E_z\log(1-D_\theta(G_\phi(z)))$$
The complete game can be specified as:
$$\underset{\phi}{\min}\underset{\theta}{\max} g(\phi, \theta) = \mathbb E_{x\sim p_{real}}\log D_\theta(x) + \mathbb E_z\log(1-D_\theta(G_\phi(z)))$$
At equilibrium, JS divergence between the data and model distributions is minimized meaning G has converged to $P_{real}$ and D outputs 1/2 for all x.

If we plug in the optimal discrinimator
$$
D^* = \frac{p^*(x)}{p^*(x)+q_{\phi}(x)}
$$
The loss $\mathcal L(\theta^*, \phi) = 2\mathrm{JS}(p^*\|q_\phi)-\log(4)$.

!! Understanding GAN

* Ian's [[Generative Adversarial Network Talk]]
* [[OpenAI's survey|https://openai.com/blog/generative-models/]]
* [[What Arora thinks about GANs|GANs, Some Open Questions]]
* [[How to train a GAN]]
* [[The Numerics of GANs]]

!! Research Directions
* [[GAN Stablizing Strategies]]
* Generate HQ images. Image is a good field for generative models, because we are sensible to image plausibility.
** Higher resolution images can be generated in a multiscale fashion.
* Address GAN training difficulties
** [[Amortised MAP Inference for Image Super-resolution|https://arxiv.org/abs/1610.04490]]
** [[Towards Principled Methods for Training Generative Adversarial Networks|https://arxiv.org/abs/1701.04862]]
** When training GAN, we collpase distribution dimensionalities, but when the 2 distributions does not intersect, we get problem training. So we convolve the distribution and later recover the full distribution.
* Leverage Kullback-Leibler divergence
** Do maximum-likelihood estimation in a likelihood free setting. Similar to approximate Bayesian computation.
** The reverse of KL divergence, use GAN in Variational inference.
* More applications
** Text (large discrete ouput spaces)
** Reinforcement learning.
* Reducing variance?
** Many networks like GAN are stochastic, depends on a non-stationary sampling
** Look into RL literatures where value distribution depends on policy.

! Bibs

!! Extensions

* [[Unsupervised representation learning with deep convolutional generative adversarial networks]]
** Laplacian Pyramid of GANs is further suggested to generate high quality image in a coarse-to-fine manner.
* [[DCGAN|Unsupervised representation learning with deep convolutional generative adversarial networks]]: a stable family of architecture in training.
* Sequential
** [[SeqGAN]]
* [[Stacked GAN|https://arxiv.org/abs/1612.04357]]
* [[Super-Resolution GAN|https://arxiv.org/abs/1609.04802]]
* [[InfoGAN]]
* LS-GAN enforces $D_\theta(G_\phi(z)) - D_\theta(x)\ge\delta(x, G_\phi(z))$ and @@color:#859900;impose a Lipschitz constraint on both G and D using weight decay@@. This leads to D having non-vanishing gradients everywhere between all real and fake sample pairs.
* [[WGAN]] uses Earth-Mover distance as the objective. This requires the discriminator to be a Lipschitz function and they use weight clipping to achieve that.
* [[Improved WGAN|https://arxiv.org/abs/1704.00028]] imposes $\|\nabla_{\hat x}D_{\theta}(\hat x)\|_2\approx 1$ where $\hat x = \epsilon x+(1-\epsilon)G_\phi(z)$. This leads to D having norm 1 gradients everywhere between all real and fake sample pairs in the limit.
* [[CycleGAN and pix2pix|https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix]]: pretty good image effects.
* [[Progressive Growing of GANs for Improved Quality, Stability, and Variation|http://research.nvidia.com/publication/2017-10_Progressive-Growing-of]]
** Good results
* [[SN-GAN]] learns ImageNet
* [[StarGAN]]
* [[BigGAN|https://arxiv.org/abs/1809.11096]]

!! Theory

* [[Divergence Classed GANs]]
* [[Density Ratio GANs]]

!! Applications
* Text Generation
** [[MaskGAN]]
* [[Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling|https://arxiv.org/abs/1610.07584]]
* [[Semi-Supervised Learning with Generative Adversarial Networks|http://arxiv.org/abs/1606.01583]]
** Better images than DCGAN are generated adding the classification label
* [[Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network]]
* [[Learning to Pivot with Adversarial Networks|https://arxiv.org/abs/1611.01046]]
** joint training adversarial networks to constrain the predictive model to be robust on nuisance parameter.

!! Talks
* [[How to train a GAN]]
* [[DALI 2017 GAN Workshop]]