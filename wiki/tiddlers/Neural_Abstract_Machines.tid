created: 20170502084841487
modified: 20180512095942631
tags: [[Sequential Models]]
title: Neural Abstract Machines
type: text/vnd.tiddlywiki

! Bibs
* NTM
* [[Lie-Access NTM]]
* [[Differentiable Neural Computer]]
* [[Neural Map: Structured Memory for Deep Reinforcement Learning|https://arxiv.org/abs/1702.08360]]: DNC for 3D navigation RL.

! Remarks
* Unified memory is not enough
** NTM: could only 'allocate' memory in contiguous blocks, leading to management problems
** DNC adds links and usage (good enough?)
** Sparse Access Memory works for both NTM and DNC, scale up?

! Applications
The model must interact with a symbolic executor through non-differentiable operations to search over a large program space. 

Differentiable communication? [[Gumbel-Softmax|https://arxiv.org/abs/1611.01144]] trick can to approximate discrete communication decisions with a continuous representation during training.

* [[Semantic Parsing]]
* Meta-Learning: [[Meta-Learning with Memory Augmented Neural Networks]]

!! Program Induction
Neural Program Synthesis Tasks: Copy, Grade-school addition, Sorting, Shortest Path. 

* Neural Turing Machine
* Reinforcement Learning Neural Turing Machines
* Stack Recurrent Nets
* [[Neural Programmer]]
* Neural Programmer-Interpreter
* Neural GPU
* Learning Simple Algorithms from Examples
* [[Differentiable Neural Computer]]
* [[NPI via Recursion]]
* [[Program Induction by Rational Generation: Learning to Solve and Explain Algebraic Word Problems]]
* [[Learning Neural Programs To Parse Programs|https://github.com/liuchangacm/neuralparser]]
** predict output by learning the compiler and run the program? what is the point? (m)any applications?

!! Bibs

* [[Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision]]

!! Challenges

* Existign neural program architecutures do not generalize well.
* No Proof of Generalization