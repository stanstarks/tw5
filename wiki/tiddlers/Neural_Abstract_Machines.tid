created: 20170502084841487
modified: 20180518071526294
tags: [[Program Induction]] [[Sequential Models]]
title: Neural Abstract Machines
type: text/vnd.tiddlywiki

! Bibs
* NTM
* [[Lie-Access NTM]]
* [[Differentiable Neural Computer]]
* [[Neural Map: Structured Memory for Deep Reinforcement Learning|https://arxiv.org/abs/1702.08360]]: DNC for 3D navigation RL.

! Remarks
* Unified memory is not enough
** NTM: could only 'allocate' memory in contiguous blocks, leading to management problems
** DNC adds links and usage (good enough?)
** Sparse Access Memory works for both NTM and DNC, scale up?

! Applications
The model must interact with a symbolic executor through non-differentiable operations to search over a large program space. 

Differentiable communication? [[Gumbel-Softmax|https://arxiv.org/abs/1611.01144]] trick can to approximate discrete communication decisions with a continuous representation during training.

* [[Semantic Parsing]]
* Meta-Learning: [[Meta-Learning with Memory Augmented Neural Networks]]
* [[Program Induction]]

!! Bibs

* [[Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision]]

!! Challenges

* Existign neural program architecutures do not generalize well.
* No Proof of Generalization