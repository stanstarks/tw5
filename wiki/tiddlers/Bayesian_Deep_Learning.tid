created: 20170621062553683
modified: 20200220013733486
tags: [[Probabilistic Deep Learning]] Bayesian
title: Bayesian Deep Learning
type: text/vnd.tiddlywiki

! History
First Bayesian NNs are training with Laplace's method. Neal uses HMC. In late 90s, VI is used.

Modern VI relied on a fully factorized approximation, assuming independence of each weight scalar in each layer from all other weights.

Expectation propagation is recently studied, relying on various forms of Renyi's $$\alpha$$-divergence. Seems to sacrifice their estimation of the dominant modes of the posterior.

! Challenges

* difficulty of reasoning about choice of the prior $$P(\mathbf W)$$
* intractability of posterior inference

! Introduction

Bayesian deep learning is

<<<
Posterior inference of layered representations of high-dimensional data
<<< Ranganath+, AISTATS 2015

Topics

* Deep exponential families: $$z_{n,l,k}\sim \text{Exp-Fam}(g(w^\top_{l,k}z_{n,l+1}))$$

! Bibs
* Recovering tractable distributions of weights
** ICML 2015 Weight uncertainty in neural networks
* Sampling based
** NIPS 2016 DISCO nets: DISsimilarity COefficient networks

! Algorithms

* [[Assumed Density Filtering]]
** [[Probabilistic Backpropagation]]
** [[Elastic Weight Consolidation]]

! Code

* [[Bayesian Networks|https://github.com/JavierAntoran/Bayesian-Neural-Networks]]

! Resources

* [[NIPS19 Bayesian Deep Learning Workshop]]

! Tasks

* [[Bayesian Object Detection]]
* [[Bayesian Diagnosis]]