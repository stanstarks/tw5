created: 20161012083717362
modified: 20180906043434854
tags: [[Recurrent Neural Networks]]
title: Sequential Models
type: text/vnd.tiddlywiki

! Models

* [[Hopfield Net]]
* [[LSTM|Long short-term memory]]
* Tapes
* Arrays
* Stacks
* [[Stochastic Recurrent Network]]
* [[Neural HSMM]]

* Restrictions on recurrent weight matrix
** uRNN
** scoRNN

!! Advanced Structures
* [[Multi-Scale RNN]]
* [[Connectionist Temporal Classification]] loss
* Faster RNN
** [[Quasi-RNN]]
** [[SRU]]
* [[Attention Mechanism]]
* Memory networks
** External Memory
*** [[Memory Networks]]
*** [[Neural Machine Translation by Jointly Learning to Align and Translate|https://arxiv.org/abs/1409.0473]]
*** (Stack) RNNs with attention
*** Dynamic Mem. Nets
* [[Neural Abstract Machines]]
* Associative memory
** [[Using Fast Weights to Attend to the Recent Past]]
* Adaptive Computation Time
** [[Adaptive Computation Time for Recurrent Neural Networks|https://arxiv.org/abs/1603.08983]]
** [[tf implementation|https://github.com/DeNeutoy/act-tensorflow]], pretty slow
* Neural Programmer

! Bibs

* [[Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets]]
* [[Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision|https://arxiv.org/abs/1611.00020]]
** non-differentiable memory optimized with REINFORCE, improved weakly supervised semantic parsing on WebQuestionsSP.
* [[Differentiable neural computers|https://deepmind.com/blog/differentiable-neural-computers/]]
** DeepMind's nature paper
* [[Tracking the World State with Recurrent Entity Networks|https://openreview.net/forum?id=rJTKKKqeg]]
** [[tensorflow implementation|https://github.com/jimfleming/recurrent-entity-networks]]
** the results aren't easily comparable to DNC

Most networks with external memory use some form of ''content-based'' memory access, find the memory closest to some ''key vector'' emitted by the network, return either the memory content or an associated ''value vector''.

* [[MemN2N|http://arxiv.org/abs/1604.06045]]
* [[Can Active Memory Replace Attention?|https://arxiv.org/abs/1610.08613]]
** Extended Neural GPU with an output tape tensor $p$, which has access to @@color:#859900;all previous outputs@@.
** Use teacher forcing in training.

! Tools
* [[LSTMVis|https://github.com/HendrikStrobelt/LSTMVis]]