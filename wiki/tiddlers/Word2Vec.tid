created: 20160425022459698
modified: 20180531062902425
tags: [[Stanford NLP Course]]
title: Word2Vec
type: text/vnd.tiddlywiki

! Ideas to try

* character level word embedding learning
** learn word embedding model end2end to avoid storing large stuff 
** probably better word root stuff?
* definition based word embedding learning

! refs

* [[Distributed Representations of Words and Phrases and their Compositionality|https://arxiv.org/abs/1310.4546]]
* [[Enriching Word Vectors with Subword Information|https://arxiv.org/abs/1607.04606]]
* [[Bag of Tricks for Efficient Text Classification|https://arxiv.org/abs/1607.01759]]
** [[fastText supervised tutorial|https://github.com/facebookresearch/fastText/blob/master/tutorials/supervised-learning.md]]
** Language detector in less than 1MB

!! Similarity to Count-Based Models
* [[Neural Word Embedding as Implicit Matrix Factorization|https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization]]: Word2Vec == PMI matrix factorization of count based models
* Count based and most neural models have equivalent performace when properly hyperoptimized.

Word similarity is hard to represent. High-dim vectors are not robust. Usually around 25-1000 dim.

Represent words as vectors in some space. Learn high-quality word vectors from hugh datasets with billions of words, and with millions of words in the vocabulary.

king:queen=man:woman

''The distributional hypothesis'': words with similar context have similar meaning.

We can represent words as cooccurence matrix. And reduce dim (by e.g. svd). This method suffer from frequency inbalance, computation inefficiency, etc.

!! Idea
By predicting words to the left and right, cooccurence will also be captured.

!! The model
Predicting surrounding words. The object of Skip-gram model is to maximize the average log probability:
$$
J(\Omega) = \frac 1 T\sum_{t=1}^T\sum_{-c\le j\le c, j\ne0}\log p(w_{t+j}|w_t)
$$
Where the input $w_t$ is the center word of a 1-of-V (or one-hot) encoding. The basic Skip-gram formulation defines $$p(w_{t+j}|w_t)$$ as softmax, whose computation is expensive when vocabulary is large (10^^6^^). 

!!! Hierarchical softmax
$$
p(w|w_I) = \prod_{j=1}^{L(w)-1}\sigma([n(w, j+1)=ch(n(w,j))]\cdot v_{n(w,j)}'\top v_{w_I})
$$
where $$v$$ and $$v'$$ are the same as $$v_{in}$$, $$v_{out}$$, $$\sigma(x)=1/(1+\exp(-x))$$, $$[x]$$ is 1 if $$x$$ is true otherwise -1.

So the nagative sampling approximates it as:
$$
\log p(w_O|w_I)\approx\log\sigma(\langle v^{w_I}_{in}, v^{w_O}_{out}\rangle)+\sum_{k=1}^K\mathbb E_{wk\sim P_n(w)}[\log \sigma(-\langle v^{w_I}_{in}, v^{w_O}_{out}\rangle)],
$$
where $$\sigma(x)=\frac 1 {1+\exp(-x)}$$ is the sigmoid (logistic) function, and the expectations are computed by drawing random words.
