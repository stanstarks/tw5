created: 20170905064640805
modified: 20180224235826458
tags: NLP
title: Word Embedding
type: text/vnd.tiddlywiki

* BoW
* Distributed word embeddings
* Continuous ''vector'' representation of words
** co-occurrence stats to obtain vectors for long phrases
** LSA, fail to preserve linear regularities among words
** LDA, computationally expensive on large datasets
** [[Word2Vec]]
** [[GloVe|Global Vectors for Word Representation]]
** CoVe
** CoVeR: considers covariate
* [[Hashed Character $n$-gram|https://arxiv.org/abs/1511.06018]]

! Refs
* [[Supervised Learning of Universal Sentence Representations from Natural Language Inference Data|https://arxiv.org/abs/1705.02364]]
** embedding sentences on Stanford Natural Language Inference dataset
** BiLSTM with attention, max-pooling over hidden representation
* [[One-shot and few-shot learning of word embeddings|https://arxiv.org/abs/1710.10280]]