created: 20170427153639938
modified: 20171220082548684
tags: [[Probabilistic Models]]
title: Statistical Measures
type: text/vnd.tiddlywiki

We want to learn a true distribution $Q$, which can be observed from i.i.d. samples. We have a parametric class of models $\mathcal P$ and we have parameters identify an individual model $P$. The task is to navigate through the class of models and find a good one.

We assume $P$ is capable of:

* sample from, (random field models, graphical models are examples cannot sample from)
* compute parameter gradient w.r.t. samples
* strongest assumption is we can compute the likelihood

[[GAN|Generative Adversarial Network]] is a likelihood-free model which can be sampled from. Because many possible input could be mapped to the same output. To compute a pointwise likelihood, we have to find the preimage, which is not tractable.

! Measure the distance

!! Integral Probability Metrics

* Muller, 1997
* Sriperumbunder et al., 2010

Taking the supremum over a difference of expectations. 
$$
\gamma_{\mathcal F}(P, Q) = \underset{f\in\mathcal F}{\sup}\left|\int fdP-\int fdQ\right|
$$

Depending on the structure of $\mathcal F$, we get different measures. 

* Energy statistics [Szekely, 1997]
* $\mathcal F$ is a unit-ball in a RKHS $\mathcal H$: Kernel [[MMD|Maximum Mean Discrepancy]]
* Wasserstein distance [Cuturi, 2013]
* [[DISCO Net]]s [Bouchacourt et al. 2016]

**Wasserstein Distance**:
$$
W_c(P\|Q):=\inf_{\rho:\rho_X=P, \rho_Y=Q}\int\int c(x, y)d\rho(x, y)
$$

Because we use integral here, we can compute this distance with samples.

!! Proper scoring rules

* [Gneiting and Raftery, 2007]

We take expectation of a score of how well the distribution $P$ fits the realization $x$.
$$
S(P, Q) = \int S(P, x)dQ(x)
$$
$$
S(P, P) \ge S(P, Q), \forall P, Q\in\mathcal P
$$
The scores can be used are:

* log-likelihood, $S(P, x)=\log P(x)$ then this is maximum likelihood
* Bayes score
* Quadratic score, $S(P, x) = 2P(x)-\|P\|^2_2$
* Pseudoshperical score, $S(P, x) = P(x)^{\alpha-1}/\|P\|_{\alpha}^{\alpha-1}$

!! $f$-divergence

* [Ali and Silvey, 1966]
* [[Divergence Classed GANs]]

$f$-divergence is a generalization of KL divergence. Now we require knowing the distribution of both $P$ and $Q$
$$
D_f(P\|Q) = \int f\left(\frac{dP(x)}{dQ(x)}\right)dQ(x)
$$
where $f$ is a convex function of likelihood ratio. $f: \mathbb R\rightarrow\mathbb R\cup\{+\infty\}$. Properties:

# $D_f(P\|Q)\ge 0$ and $D_f(P\|Q) = 0$ when $P=Q$;
# $D_f(P\|Q)=D_g(P\|Q)$ for any $g(x) = f(x) + \lambda(x-1)$
# Taking $f_0(x) := f(x)-(x-1)f'(1)$ is good. It's nonnegative, nonincreasing on $(0, 1]$ and nondecreasing on $[1, \infty)$;
# Taking the convec restriction $f_+(x) = f(x)$ if $x\ge0$ and $\infty$ otherwise.

Examples are

* [[Kullback-Leibler divergence]]: $f = x\log x$
* Reverse KL: $f(x) = -\log x$
* [[Jensen-Shannon divergence]]: $f(x) = -(x+1)\log\frac{x+1}{2}+x\log x$
* Total variation: $f(x) = |x-1|$
* Pearson $\chi^2$

Direct MLE 
$$
\max_{\theta\in\mathbb R^d}\frac1m\sum_{i=1}^m\log P_\theta(x^i)
$$
is equivalent to minimizing the KL-divergence $KL(P_r\|P_\theta)$.

! Bibs
* [[Parallel Multiscale Autoregresssive Density Estimation]]