created: 20171117035730065
modified: 20171118091143770
tags: [[Inference Methods]]
title: Evolution Stragegies
type: text/vnd.tiddlywiki

My $f(\theta)$ is easy to evaluate but hard to optimize, perhaps because it contains discrete operations, or the function is piecewhise constant. Backpropagation is not possible.

Observe that for any probability distribution $p_\psi$ over $\theta$ the following holds:
$$
\min_\theta f(\theta)\le\min_\psi\mathbb E_{\theta\sim p_\psi}f(\theta)
$$
therefore in ES we concentrate of the following optimization problem instead:
$$
\psi^*\rightarrow \arg\min_\psi\mathbb E_{\theta\sim p_\psi}f(\theta)
$$
Often, depending on the function $f$ and class of distribution $p_\psi$, a local minimum of $f$ can be recovered from a local minimum of $\psi$.

[[REINFORCE]] gradient estimator relies on the following trick
$$
\frac{\partial}{\partial \psi_i}\mathbb E_{\theta\sim p_\psi}[f(\theta)]=\mathbb E_{\theta\sim p_\psi}[\frac{\partial}{\partial \psi_i}\log p_\psi(\theta)f(\theta)]
$$