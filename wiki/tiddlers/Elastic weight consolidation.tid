created: 20170317104352496
modified: 20190801023333254
tags: [[Bayesian Deep Learning]] [[Overcoming Catastrophic Forgetting]] [[Laplace Approximation]]
title: Elastic Weight Consolidation
type: text/vnd.tiddlywiki

[[link|http://www.pnas.org/content/early/2017/03/13/1611835114.abstract]]

Elastic weight consolidation is best described as online sequential (diagonalised) Laplace approximation. This is similar to [[Assumed Density Filtering]] the precursor to expectation-propagation. Laplace approximation takes a probability density $$p$$ and approximates it with a Gaussian. By Bernstein-von Misestype convergence theorems, the posterior will converge to the Laplace. EWC use a diagonalised Hessian for variance estimation.

For the first task, the Hessian would be the Fisher information from task $$A$$, $$F^A$$, plus the Hessian of the log prior $$\log p(\theta)$$.

Bayesian inference wouldn't suffer from catastrophic forgetting which haunts optimization-based methods. But the full posterior is intractable. 

* EWC approximated Bayesian computation

The problem is begins at the third task, the right approximation
$$
\log p(\theta|\mathcal D_A, \mathcal D_B)\approx -\sum_i(F^A+F^B)_{i, i}(\theta_i-\theta_i^{A, B})^2
$$

The penalty around $$\theta^A$$ has already been taken account.

EWC further required the knowledge of the task it is performing. The tasks here can be different minibatches of the same task.