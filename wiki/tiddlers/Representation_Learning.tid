created: 20161014064037895
modified: 20171115012745650
tags: [[Learning Theory]]
title: Representation Learning
type: text/vnd.tiddlywiki

! Theory
!! Curse of Dimensionality
In a finite-dimensional, bounded space, all metrics are equivalent:

<<<
For each $x\in\Omega$, exists constants $c, C$ such that $\forall x'\in\Omega$, $cd(x, x')\le\tilde d(x, x')\le Cd(x, x')$.
<<<

But as the dimension increases, metrics start to diverge. In particular, the Euclidean distance in high-dimensional spaces is typically a poor measure of similarity for practical purposes. Local decisions around training do not extend to the whole space.

To avoid the curse of dimensionality, we need a guiding principle that plays well with our data (images, sounds, etc.) We need features that linearize intra-class variability and perserve inter-class variability.

!! Generalization Error
It is easy to construct discriminative features:

* Using a Gaussian RBF, it suffices to let $\sigma\rightarrow0$.
* The estimator converges to the nearest neighbor classifier:
$$
\hat f(x) = f(x_{i(x)}), i(x) = \arg\underset{i}{\min}\|x-x_i\|
$$

While it may be easy to correctly classify our training examples, we do not necessarily improve our generalization error:
$$
\mathbb E_{(x, y)}(\mathcal l(\hat f(x), y))
$$

!! Linearization
We want to obtain a representation $\Phi(x)$ such that
$$
\hat f(x) = \text{sign}(a^\top\Phi(x)+b)
$$
is a good approximation of $f(x)$. Thus $f(x)$ is approximately linearized by $\Phi(x)$:
$$
f(x) \approx \text{sign}(a^\top\Phi(x)+b)
$$
In particular, we should have
$$
a^\top(\Phi(x)-\Phi(x') = 0 \Rightarrow f(x)=f(x').
$$

!! Invariance and Symmetry
A global symmetry is an operator $\varphi\in Aut(\Omega)$ that leaves $f$ invariant:
$$
\forall x\in\Omega, f(\varphi(x)) = f(x)
$$
They can be absorbed by $\Phi$ to varying degrees:

* ''Invariants'': $\Phi(\varphi(x)) = \Phi(x)$ for each $x$.
* ''Covariants'': $\Phi(\varphi(x)) = A_\varphi\Phi(x)$ for each $x$, where $A_\varphi$ is "simpler" than $\varphi$.

[[Image Representation Learning]]

! Algorithms
* [[t-Distributed Stochastic Neighbor Embedding]]
* [[Random Projection]]