created: 20170208074243139
modified: 20171114234416124
tags: [[Generative Adversarial Network]]
title: WGAN
type: text/vnd.tiddlywiki

* [[Wasserstein GAN|https://arxiv.org/abs/1701.07875]]
* [[Towards Principled Methods for Training Generative Adversarial Networks|https://arxiv.org/abs/1701.04862]]
* [[Improved Training of Wasserstein GANs|https://arxiv.org/abs/1704.00028]]
** A simple way to enforce the Lipschitz constraint on the class of functions, which can be modeled by the neural network, is weight clipping. It was proposed that training can be improved by instead augmenting the loss by a regularization term that penalizes the deviation of the gradient of the critic (as a function of the network's input) from one.
* [[On the regularization of Wasserstein GANs|https://arxiv.org/abs/1709.08894]]
** A theoretical proof of using a weaker regularization term enforcing the Lipschitz constraint is preferable

! Implementations

* [[tf|https://github.com/igul222/improved_wgan_training]]

! Idea
Directly learning $P_\theta$ is bad because it'll be very unlikely that all of $P_r$ lies within $P_\theta$'s support. We can add random noise to $P_\theta$.

! Details

WGAN uses a critic instead of a discriminator. The critic is trained to provide an approximation of the Wasserstein distance between the real data distribution $P_r$ and the generator distribution $P_g$. The approximation is based on the Kantorovich-Rubinstein duality (see [[this blog|https://vincentherrmann.github.io/blog/wasserstein/]] for more):
$$
W(P_r, P_g)\propto \underset{f}{\max}\mathbb E_{x\sim P_r}[f(x)]-\mathbb E_{x\sim P_g}[f(x)]
$$

[[Optimal Transport GAN and VAE Explaination|https://arxiv.org/abs/1706.01807]]

! Experiments
Strided convolution creates checkerboard patterns and introduced