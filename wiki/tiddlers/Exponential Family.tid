created: 20141017074130201
modified: 20200603082501029
tags: Statistics
title: Exponential Family
type: text/vnd.tiddlywiki

!Definition

$$
q(\theta)\propto\exp[\lambda^TT(\theta)]
$$

$$\lambda$$ is the natural parameter and $$T(\theta)$$ is the sufficient statistics. $$\mu:=\mathbb E_q[T(\theta)]$$ is the expectation parameter.

In the Gaussian case:

$$
\begin{aligned}
\mathcal N(\theta|m, S^{-1})&\propto \exp[-\frac12(\theta-m)^TS(\theta-m)]\\
&\propto[(Sm)^T\theta + \operatorname{Tr}(-\frac{S}{2}\theta\theta^T)]
\end{aligned}
$$

* Gaussian distribution: $$q(\theta):=\mathcal N(\theta|m, S^{-1})$$
* Natural parameters: $$\lambda:=\{Sm, -S/2\}$$
* Expectation parameters: $$\mu:=\mathbb E_q(\theta), \mathbb E_q(\theta\theta^T)\}$$


!! Single-parameter
$$
f_X(x|\theta) = h(x) \exp \left (\eta(\theta) \cdot T(x) -A(\theta)\right )
$$
where $$T(x)$$, $$h(x)$$, $$\eta(\theta)$$, and $$A(\theta)$$ are known functions.

In the definitions above, the functions $$T(x)$$, $$\eta(\theta)$$ and $$A(\eta)$$ were apparently arbitrarily defined. However, these functions play a significant role in the resulting probability distribution.

* $$T(x)$$ is a sufficient statistic of the distribution. For exponential families, the sufficient statistic is a function of the data that fully summarizes the data $$x$$ within the density function. This means that, for any data sets $x$ and $y$, the density value is the same if $$T(x) = T(y)$$. This is true even if $x$ and $y$ are quite differentâ€”that is,  $d(x,y)>0$. The dimension of $$T(x)$$ equals the number of parameters of $$\theta$$ and encompasses all of the information regarding the data related to the parameter $$\theta$$. The sufficient statistic of a set of independent identically distributed data observations is simply the sum of individual sufficient statistics, and encapsulates all the information needed to describe the posterior distribution of the parameters, given the data (and hence to derive any desired estimate of the parameters). This important property is further discussed below.
* $$\eta$$ is called the natural parameter. The set of values of $\eta$ for which the function $$f_X(x;\theta)$$ is finite is called the natural parameter space. It can be shown that the natural parameter space is always convex.
* $$A(\eta)$$ is called the log-partition function because it is the logarithm of a normalization factor, without which $$f_X(x;\theta)$$ would not be a probability distribution ("partition function" is often used in statistics as a synonym of "normalization factor"):
$$
A(\eta) =  \ln\left ( \int_x h(x) \exp (\eta(\theta) \cdot T(x)) \operatorname{d}x \right )
$$

The function $$A$$ is important in its own right, because the mean, variance and other moments of the sufficient statistic $$T(x)$$ can be derived simply by differentiating $$A(\eta)$$. For example, because $$\ln(x)$$ is one of the components of the sufficient statistic of the gamma distribution, $$\mathbb{E}[\ln x]$$ can be easily determined for this distribution using $$A(\eta)$$. Technically, this is true because
$$
K(u|\eta) = A(\eta+u) - A(\eta),
$$
is the cumulant generating function of the sufficient statistic.

! Properties
Exponential families have a large number of properties that make them extremely useful for statistical analysis. In many cases, it can be shown that, except in a few exceptional cases, only exponential families have these properties. Examples:

* Exponential families have sufficient statistics that can summarize arbitrary amounts of independent identically distributed data using a fixed number of values.
* Exponential families have conjugate priors, an important property in Bayesian statistics.
* The posterior predictive distribution of an exponential-family random variable with a conjugate prior can always be written in closed form (provided that the normalizing factor of the exponential-family distribution can itself be written in closed form). Note that these distributions are often not themselves exponential families. Common examples of non-exponential families arising from exponential ones are the Student's t-distribution, beta-binomial distribution and Dirichlet-multinomial distribution.
* In the mean-field approximation in variational Bayes (used for approximating the posterior distribution in large Bayesian networks), the best approximating posterior distribution of an exponential-family node (a node is a random variable in the context of Bayesian networks) with a conjugate prior is in the same family as the node.

[[Geometry of exponential families]]

!! Conjugate Distributions
In the case of a likelihood which belongs to the exponential family there exits a conjugate prior, which is often also in the exponential family. A conjugate prior $\pi$ for the parameter $\eta$ of an exponential family

$$
f(x|\boldsymbol\eta) = h(x) \exp \left ( {\boldsymbol\eta}^{\rm T}\mathbf{T}(x) -A(\boldsymbol\eta)\right )
$$
is given by
$$
p_\pi(\boldsymbol\eta|\boldsymbol\chi,\nu) = f(\boldsymbol\chi,\nu) \exp \left (\boldsymbol\eta^{\rm T} \boldsymbol\chi - \nu A(\boldsymbol\eta) \right ),
$$
or equivalently
$$
p_\pi(\boldsymbol\eta|\boldsymbol\chi,\nu) = f(\boldsymbol\chi,\nu) g(\boldsymbol\eta)^\nu \exp \left (\boldsymbol\eta^{\rm T} \boldsymbol\chi \right ), \qquad \boldsymbol\chi \in \mathbb{R}^s
$$
where $s$ is the dimension of $\boldsymbol\eta$ and $\nu > 0$ and $\boldsymbol\chi$ are hyperparameters (parameters controlling parameters). $v$ correspinds to the effective number of observations that the prior distribution contributes, and $\boldsymbol\chi$ corresponds to the total amount that these pseudo-observations contribute to the sufficient statistic over all observations and pseudo-observations.

We can then compute the posterior as follows:
$$
\begin{aligned}
p(\boldsymbol\eta|\mathbf{X},\boldsymbol\chi,\nu)& \propto p(\mathbf{X}|\boldsymbol\eta) p_\pi(\boldsymbol\eta|\boldsymbol\chi,\nu) \\
&= \left(\prod_{i=1}^n h(x_i) \right) g(\boldsymbol\eta)^n \exp\left(\boldsymbol\eta^{\rm T} \sum_{i=1}^n \mathbf{T}(x_i)\right)
f(\boldsymbol\chi,\nu) g(\boldsymbol\eta)^\nu \exp(\boldsymbol\eta^{\rm T} \boldsymbol\chi) \\
&\propto g(\boldsymbol\eta)^n \exp\left(\boldsymbol\eta^{\rm T}\sum_{i=1}^n \mathbf{T}(x_i)\right) g(\boldsymbol\eta)^\nu \exp(\boldsymbol\eta^{\rm T} \boldsymbol\chi) \\
&\propto g(\boldsymbol\eta)^{\nu + n} \exp\left(\boldsymbol\eta^{\rm T} \left(\boldsymbol\chi + \sum_{i=1}^n \mathbf{T}(x_i)\right)\right)
\end{aligned}
$$

! Examples

# normal
# exponential
# log-normal
# [[gamma|Gamma Distribution]]
# chi-squared
# beta
# Dirichlet
# Bernoulli
# categorical
# Poisson
# geometric
# inverse Gaussian
# von Mises-Fisher