created: 20150213071121294
modified: 20200101062012906
tags: Analysis Statistics [[Reinforcement Learning]]
title: Fisher information metric
type: text/vnd.tiddlywiki

Fisher information metric is the only Riemannian metric that "makes sense" for [[statistical manifolds|Statistical manifold]]:
$$
I(\theta) = \left[\int\frac{\partial\log p(x;\theta)}{\partial\theta_i}\frac{\partial\log p(x;\theta)}{\partial\theta_j}p(x;\theta)dx\right]=[g_{ij}]
$$
The [[Infinitesimal length element]] is given by
$$
ds^2 = \sum_{i=1}^d\sum_{j=1}^dd\theta_i^T\nabla^2F(\theta)d\theta_j
$$

Cencov proved that for a non-singular transformation of the parameters $\lambda=f(\theta)$, the information matrix
$$
I(\lambda)=\left[\frac{\partial\theta_i}{\partial\lambda_j}\right]I(\theta)\left[\frac{\partial\theta_i}{\partial\lambda_j}\right].
$$

Normalized and de-correlated activation is well known for improving the conditioning of the Fisher information matrix.

!! Appearances
In reinforcement learning, the natural gradient is defined as $$F^{-1}g$$, where $$F$$ is the average Fisher information matrix
$$
\begin{aligned}
F &= \mathbb E_{s, a\sim\pi}[(\nabla_\theta\log\pi_\theta(a|s))^T(\nabla_\theta\log\pi_\theta(a|s))]\\
&= \mathbb E_{s, a\sim\pi}[-\nabla^2_\theta\log\pi_\theta(a|s)]
\end{aligned}
$$
If the empirical distribution over the data is equal to the model distribution $$p_\theta(y|f(x,\theta))$$, then the Fisher, empirical Fisher and the Hessian are all equal.

!! Connections between Fisher, GGN and Hessian
The Fisher capstures paritla curvature information about the problem

!! Limitations of Empirical Fisher Approximation

* [[paper|https://arxiv.org/abs/1905.12558]]