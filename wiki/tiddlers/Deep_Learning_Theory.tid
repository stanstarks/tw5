created: 20161012084433429
modified: 20180221075844771
tags: [[Deep Learning]]
title: Deep Learning Theory
type: text/vnd.tiddlywiki

! Dealing non-linearity
Nonlinearity of problems like XOR popularized [[non-linear hidden layers|http://dl.acm.org/citation.cfm?id=104293]] and kernel methods.

Another well-known example is [[the parity problem|https://en.wikipedia.org/wiki/Perceptrons_(book)]]: although a single non-linear hidden layer is sufficient to represent any function, it is not guaranteed to represent it efficiently, can even require exponentially many more parameters than a deeper model.

! [[Deep Approximiation Theory]]

! Estimation Theory
''Theorem'' [Barron'92] The mean integraded square error between the estimated network $\hat F$ and the target function $f$ is bounded by
$$
O\left(\frac{C_f^2}{N}\right) + O\left(\frac{Nm}{K}\log K\right),
$$
where $K$ is the number of training points, $N$ is the number of neurons, $m$ is the input dimension, and $C_f$ measures the global smoothness of $f$.

This formula combines approximation and estimation error. We cannot explain why @@color:#dc322f;online/stochastic optimization works better than batch normalization@@. 

! Plausible Optimization
* [[On Optimization in Deep Learning]]
* [[Rethinking Generalization]]

! Statistical Learning Theory
The complexity or capacity of NNs can be computed by measuring how many configurations can be shattered (VC-dimension)

The capacity of the network, if measured by the number of pieces in a piecewise linear approximation, increases exponentially with depth. [Montufar, Pascanu et al, '14]

These results quantify an upper bound on the empirical risk of deep neural networks. But the bounds might be very pessimistic. We still have to figure out @@color:#dc322f;the superior generalization properties of CNNs versus models with similar capacity@@. 

There is a functional equivalence between models of different depths at equal capacity [Ba and Caruana '14].

! Information Theory View
* [[Information Theory for Deep Learning]]
* [[Generalization of Deep Nets]]

! Understanding DNN

* [[Opening the Black Box of Deep Neural Networks via Information|https://arxiv.org/abs/1703.00810]]
* [[Energy Propagation in Deep Convolutional Neural Networks|https://arxiv.org/abs/1704.03636]]
* [[Exploring loss function topology with cyclical learning rates|https://arxiv.org/abs/1702.04283]]

! Interpretability
* [[Deep learning in biology]]
* DeepMind: [[Interpreting Deep Neural Networks using Cognitive Psychology|https://deepmind.com/blog/cognitive-psychology/]]