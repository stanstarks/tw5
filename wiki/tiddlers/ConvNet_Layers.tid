created: 20151112065545766
modified: 20180330024259552
tags: [[Convolutional Neural Network]]
title: ConvNet Layers
type: text/vnd.tiddlywiki

* [[Convolutional Layer]]
* [[Pooling Layer]]
* [[Nonlinearity Layer]]
* [[Metric Learning]]

!! [[Normalization Layer]]

!! Local Response Normalization(LRN)
The local response normalization layer performs a kind of ''lateral inhibition'' by normalizing over local input regions. In `ACROSS_CHANNELS` mode, the local regions extend across nearby channels, but have no spatial extent (i.e., they have shape `local_size x 1 x 1`). In `WITHIN_CHANNEL mode`, the local regions extend spatially, but are in separate channels (i.e., they have shape `1 x local_size x local_size`). Each input value is divided by $(1+(\alpha/n)\sum_ix^2_i)^\beta$, where $n$ is the size of each local region, and the sum is taken over the region centered at that value (zero padding is added where necessary).

!! Fully-connected
This is what is used in traditional neural networks. It gives the network more flexibility to convert a FC to CONV, allowing us to 'slide' the original ConvNet across a larger image.

!! [[Dropout]]

!! Loss
We can use different loss functions for different tasks. 

''Sigmoid cross-entropy loss'' is used for predicting K independent probability values in [0,1]. In logistic regression, our hypothesis took the form:
$$
h_\theta(z) = \frac{1}{1+\exp(-\theta\top z)},
$$
and the model parameters $\theta$ were trained to minimize the cost function:
$$
L(\theta) = -\left(\sum_{i=1}^my_i\ln\log h_\theta(z_i)+(1-y_i)\log(1-h_\theta(z_i))\right)
$$

Softmax loss is used for predicting a single class of $K$ mutually exclusive classes. Our hypothesis takes the form:
$$
h(\mathbf z, j) = \frac{e^{z_j}}{\sum_{k=1}^Ke^{z_k}},\qquad j = 1, \dots, K,
$$
and the loss function will be:
$$
J(z) = -\left(\sum_{i=1}^m\sum_{k=1}^K1_{y_i = k}\log h(z, k)\right).
$$
which is a generalized case of logistic regression loss. (When $K=2$, the softmax reduces to logistic regression).

Since the function maps a vector and a specific index i to a real value, the derivative needs to take the index into account. For the cross entropy cost, we have:
$$
\frac{\partial L}{\partial z_k} = -\sum_{i=1}^m(1_{y_i=k}-h(\mathbf z, k))
$$

Euclidean loss is used for regressing to real-valued labels [-inf,inf]