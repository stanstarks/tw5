created: 20161008091826135
modified: 20190119023127787
tags: [[Deep Learning Applications]]
title: Reinforcement Learning
type: text/vnd.tiddlywiki

! Tutorials

* [[WildML: Learning Reinforcement Learning|http://www.wildml.com/2016/10/learning-reinforcement-learning/]]
* [[Simple Reinforcement Learning with Tensorflow|https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0]]
* [[Deep RL Bootcamp 2017 Berkeley|http://nuit-blanche.blogspot.hk/2017/10/slides-and-videos-deep-rl-bootcamp-26.html]]
* [[An Outsider's Tour of Reinforcement Learning|http://www.argmin.net/2018/03/26/outsider-rl]]
* [[ICML17 Deep RL Tutorial|https://sites.google.com/view/icml17deeprl]]

! Basic
!! Theorems

<<<
''Theorem'' [Value iteration converges]<br>
At convergence, we have found the optimal value function $V^*$ for the discounted infinite horizon problem, which satisfies the [[Bellman equation]]
$$
\forall S\in S:\qquad V^*(s) = \max_a\sum_{s'}P(s'|s, a)[R(s, a, s')+\gamma V^*(s')]
$$
<<<

''Temporal Difference Learning:'' Policy evaluation for the current policy $\pi_k$
$$
V^{\pi_k}_{i+1}(s) \leftarrow \sum_{s'}P(s'|s, \pi_k(s))[R(s, \pi_k(s), s')+\gamma V^{\pi_k}_{i}(s')]
$$
Policy improvements: find the best action according to one-step look-ahead
$$
\pi_{k+1}(s) \leftarrow \arg\max_a\sum_{s'}P(s'|s, a)[R(s, a, s')+\gamma V^{\pi_k}(s')]
$$
Tabular methods cannot scale.

@@color:#859900;How to update it as a network?@@

<<<
''Theorem'' [Policy iteration converges]<br>
Policy iteration is guaranteed to converge and at convergence, the current policy and its value function ar the optimal policy and the optimal value function.
<<<

We sample states $\epsilon$-greedily.

!! RL agents
* value based, has a value function where policy can be inferred.
* policy based, work in decision space, more direct. Policy Graident.
* [[Actor-Critic Method]]: stores policy and value
* [[Model Based RL]]

!! Anatomy of a RL algorithm
Iterate these 3 stepsï¼š

# Generate samples (i.e. run the policy)
# Fit a model to estimate return
#* MC policy gradient: compute $\hat Q = \sum_{t'=t}^T\gamma^{t'-t}r_{t'}$
#* [[Actor-Critic Method]], [[Q-Learning]]: fit $Q_\phi(s, a)$
#* Model-based: estimate $p(s'|s, a)$
# Improve the policy
#* [[Policy Gradient]]: $\theta\leftarrow\theta+\alpha\nabla_\theta J(\theta)$
#* Q-learning: $\pi(s) = \arg\max Q_\phi(s, a)$
#* Model-based: optimize $\pi_\theta(a|s)$

!! Tricks

* Freezing target network
* Error clipping, reward clipping
* Experience replay

!! Training steps
On training half-cheetah, each one is 10x easier than previous. (each episodes is 1000 steps)

* gradient-free methods, e.g. evolution strategies, CMA, etc. 
* fully online methods, e.g. A3C, 100,000 episodes ~15 days
* policy gradient methods, e.g. TRPO, 10,000 episodes
* relay buffer value estimation, e.g. Q-learning, DDPG, NAF, etc., 1,000 episodes
* model-based deep RL, e.g. guided policy search
* model-based shallow RL, e.g. PILCO

! Bibs
* [[Faster Deep Reinforcement Learning]]
* [[Playing Doom with SLAM-Augmented Deep Reinforcement Learning|https://arxiv.org/abs/1612.00380]]
* [[Learning to Communicate|https://blog.openai.com/learning-to-communicate/]]: differentiable language for RL
* [[Curiosity-driven Exploration by Self-supervised Prediction|https://pathak22.github.io/noreward-rl/]]: training with sparse reward

! Topics
* [[Imitation Learning]]
* [[Meta-Learning]]
* [[Noisy Networks for Exploration|https://arxiv.org/abs/1706.10295]]
* [[Soft Optimality]]
* [[Hierarchical Reinforcement Learning]]
* [[Learn by Generation]]
* [[RL Return]]

! Implementations
Environments

* [[CommAI-env|https://github.com/facebookresearch/CommAI-env]]
* [[Universe|https://openai.com/blog/universe/]] with [[Gym|https://gym.openai.com/]]
* [[DeepMind Lab|https://github.com/deepmind/lab]]
* [[Facebook ELF|https://github.com/facebookresearch/ELF]]
* [[TextWorld|https://www.microsoft.com/en-us/research/project/textworld/]]

[[RL Applications]]