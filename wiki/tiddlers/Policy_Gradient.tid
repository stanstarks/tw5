created: 20170823062806969
modified: 20180916103859894
tags: [[Reinforcement Learning]]
title: Policy Gradient
type: text/vnd.tiddlywiki

! Definition
Total loss:
$$
J(\theta)=E_{\tau\sim\pi_\theta(\tau)}[r(\tau)] = \int\pi_\theta(\tau)r(\tau)d\tau
$$
policy function is assumed to be Markovian:
$$
\pi_\theta(\tau) = p(s_1)\prod_{t=1}^T\pi_\theta(a_t|s_t)p(s_{t+1}|s_t, a_t)
$$
When we take the gradient, we ignore $$p(s_1)$$ and $$p(s_{t+1}|s_t, a_t)$$ term. We make the common log trick $$\frac{\nabla x}{x}=\nabla\log x$$ to allow sampling from paths $$\tau$$:
$$
\nabla_\theta J(\theta)=E_{\tau\sim\pi_\theta(\tau)}[(\sum_{t=1}^T\nabla_\theta\log\pi_\theta(a_t|s_t))(\sum_{t=1}^Tr(s_t, a_t))]
$$
with [[REINFORCE]], we sample $$\tau^i$$ from $$\pi_\theta(a_t|s_t)$$ and iteratively update $$\theta\leftarrow\theta+\alpha\nabla_\theta J(\theta)$$. This is valid even when:

* $$r$$ is discontinuous and/or unknown.
* Sample space (of paths) is a discrete set.

Policy can be generated by a neural network:
$$
\pi_\theta(a_t|s_t) = \mathcal N(f(s_t); \Sigma)
$$
We can then do back propagation on the model. 

The problem with policy gradient is that it is hard to choose learning rate and convergence is slow due to high variance. 

We should introduce causality because policy at time $$t'$$ cannot effect reward at time $$t$$ when $$t<t'$$.

! Comparisons
* Often $$\pi$$ can be simpler than $$Q$$ or $$V$$, e.g., robotic grasp
* $$V$$: doesn't precribe actions
** Would need dynamics model (+ compute 1 Bellman back-up)
* $$Q$$: need to be able to efficiently solve $$\arg\max_u Q_\theta(s, u)$$
** Challenge for continuous / high-dimensional action spaces
*** NAF: Gu, Lillicrap, Sutskever, Levine ICML 2016
*** Input Convex NNs: Amos, Xu, Kolter arXiv 2016
*** [[Deep Energy Q|https://arxiv.org/abs/1702.08165]]: Haarnoja, Tang, Abbeel, Levine, ICML 2017

Policy optimization:

* More compatible with rich architectures (including recurrence)
* More versatile
* More compatible with auxiliary objectives

Dynamic programming:

* More compatible with exploration and off-policy learning
* More sample-efficient when they work

! Improvements
* [[AlphaGo Zero]]
* [[PPO]]

! Bibs
* Classic papers
** Simple statistical gradient-following algorithms for connectionist reinforcement learning: introduces REINFORCE algorithm
** Infinite-horizon policy-gradient estimation: temporally decomposed policy gradient
** Reinforcement learning of motor skills with policy gradients: very accessible overview of optimal baselines and natural gradient
* Deep RL
** Guided policy search: deep RL with importance sampled policy gradient
** Trust region policy optimization: deep RL with natural policy gradient and adaptive step size
** Proximal policy optimization algorithms: deep RL with importance sampled policy gradient