created: 20170823062806969
modified: 20200101055522171
tags: [[Reinforcement Learning]]
title: Policy Gradient
type: text/vnd.tiddlywiki

! Definition
Total loss:
$$
J(\theta)=E_{\tau\sim\pi_\theta(\tau)}[r(\tau)] = \int\pi_\theta(\tau)r(\tau)d\tau
$$
policy function is assumed to be Markovian:
$$
\pi_\theta(\tau) = p(s_1)\prod_{t=1}^T\pi_\theta(a_t|s_t)p(s_{t+1}|s_t, a_t)
$$
When we take the gradient, we ignore $$p(s_1)$$ and $$p(s_{t+1}|s_t, a_t)$$ term. We make the common log trick $$\frac{\nabla x}{x}=\nabla\log x$$ to allow sampling from paths $$\tau$$:
$$
\log\pi_\theta(\tau) = \log p(s_1) + \sum^T_{t=1}\log\pi_\theta(a_t|s_t)+\log p(s_{t+1}|s_t, a_t)
$$
$$
\nabla_\theta J(\theta)=E_{\tau\sim\pi_\theta(\tau)}[(\sum_{t=1}^T\nabla_\theta\log\pi_\theta(a_t|s_t))(\sum_{t=1}^Tr(s_t, a_t))]
$$
with [[REINFORCE]], also the gradient of sum is the sum of gradient. We sample $$\tau^i$$ from $$\pi_\theta(a_t|s_t)$$ and iteratively update $$\theta\leftarrow\theta+\alpha\nabla_\theta J(\theta)$$. This is valid even when:

* $$r$$ is discontinuous and/or unknown.
* Sample space (of paths) is a discrete set.

Policy can be generated by a neural network:
$$
\pi_\theta(a_t|s_t) = \mathcal N(f(s_t); \Sigma)
$$
We can then do back propagation on the model. Policy generated from a Gaussian.
$$
\log \pi_\theta(a_t|s_t) = -\frac12\|f(s_t)-a_t\|^2_\Sigma+\text{const}
$$

This also works for partially observed occasions. Because Markov property is not used.

! Further Topics
* [[Policy Gradient Reducing Variance]]
* [[Policy Gradient Off-Policy]]

! Comparisons
* Often $$\pi$$ can be simpler than $$Q$$ or $$V$$, e.g., robotic grasp
* $$V$$: doesn't precribe actions
** Would need dynamics model (+ compute 1 Bellman back-up)
* $$Q$$: need to be able to efficiently solve $$\arg\max_u Q_\theta(s, u)$$
** Challenge for continuous / high-dimensional action spaces
*** NAF: Gu, Lillicrap, Sutskever, Levine ICML 2016
*** Input Convex NNs: Amos, Xu, Kolter arXiv 2016
*** [[Deep Energy Q|https://arxiv.org/abs/1702.08165]]: Haarnoja, Tang, Abbeel, Levine, ICML 2017

Policy optimization:

* More compatible with rich architectures (including recurrence)
* More versatile
* More compatible with auxiliary objectives

Dynamic programming:

* More compatible with exploration and off-policy learning
* More sample-efficient when they work

! Improvements
* [[AlphaGo Zero]]
* [[PPO]]

! Bibs
* Classic papers
** Simple statistical gradient-following algorithms for connectionist reinforcement learning: introduces REINFORCE algorithm
** Infinite-horizon policy-gradient estimation: temporally decomposed policy gradient
** Reinforcement learning of motor skills with policy gradients: very accessible overview of optimal baselines and [[natural gradient|Natural Gradient Descent]]
* Deep RL
** Guided policy search: deep RL with importance sampled policy gradient
** Trust region policy optimization: deep RL with natural policy gradient and adaptive step size
** Proximal policy optimization algorithms: deep RL with importance sampled policy gradient