created: 20170901074915401
modified: 20180504122612558
tags: NLP
title: NLP Data
type: text/vnd.tiddlywiki

Size of vocabulary: speech 20K, PTB 50K, Google 1T

This is what Baidu used in //End-to-End Speech Recognition in English and Mandarin//

* Language model: Kneser-Ney smoothed character level 5-gram model.
* English: 850mil n-grams trained with KenLM on Common Crawl Repository
* Chinese: 2bil n-grams

! Chinese vs English
Chinese characters are more similar to English syllables than English characters. In speech, there are 14.1 chars/s in English, 3.3 chars/s in Mandarin. Shannon entropy per char 4.9 bits in English, 12.6 bits in Mandarin.

! Datasets
Lee collected [[these datasets|http://www.cs.cornell.edu/home/llee/data/]].

* [[Google Research Datasets|https://github.com/google-research-datasets]]
** [[Online Discussion|https://github.com/google-research-datasets/coarse-discourse]]

!! Language Modelling
* Penn Treebank: from WSJ, very small and has been heavily processd
* Billion Word corpusï¼š Train and test sentences from the same articles and overlap in time
* WikiText: better option

!! QA
* [[Facebook ParlAI|https://github.com/facebookresearch/ParlAI]]
** bAbI and wiki stuff
** SQuAD

!! Summarization
* [[NEWSROOM|https://summari.es/]]

!! [[Sentense Classification Datasets]]