created: 20161107021510958
modified: 20180613080734818
tags: [[Generative Adversarial Network]] [[Statistical Measures]]
title: Divergence Classed GANs
type: text/vnd.tiddlywiki

references:

* [[paper|https://arxiv.org/abs/1609.03126]]
* [[blog|http://www.inference.vc/are-energy-based-gans-actually-energy-based/]]
* [[A Connection between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models|https://arxiv.org/abs/1611.03852]]
* [[fGAN]]: generalize to $f$-divergence

! Relationship with mutual information
Original GAN objective itself can be derived from mutual information, and in fact, the discriminator $D$ can be thought of as a variational auxillary variable, exactly the same role as the recognition model $q(c|x)$ in the [[InfoGAN]] paper.
$$
\mathcal l_{GAN}(\theta) = I[x, y]
$$
The idea is simple, if $x_{real}$ and $x_{fake}$ come from the same distribution, it should be impossible to guess the value of $y$ better than chance, hence the mutual information would be 0.

! A unifying view on GAN-type algorithms

The loss for $D$, a discrepancy function $s(x)$ usually takes the following separable form:
$$
\mathcal L(s;P,Q)=\mathbb E_{x\sim P}\mathcal l_1(s(x))+E_{x\sim Q}\mathcal l_2(s(x)),
$$
In the original GAN:
$$
D(x) = \frac{1}{1+e^{-s(x)}}
$$
The training criterion for $s$ becomes:
$$
\begin{aligned}
\mathcal L(s) &= & \mathbb E_{x\sim P}\log D(x)+E_{x\sim Q}\log (1-D(x))\\
&=& \mathbb E_{x\sim P}\text{softminus}(s(x))+E_{x\sim Q}\text{softplus}(s(x))
\end{aligned}
$$

The optimal $$s$$ minimizes $$KL[Q\|P]$$. If we choose nonlinearity $$f=\text{softplus}$$ in the outer loop, we recover the GAN variant which minimizes [[Jensen-Shannon-divergence|https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence]], while choosing $f=\text{softminus}$ recovers the version the authors use in the original paper and in DCGAN.

It is also worth noticing that what teh original GAN objective trying to minimize is a lower bound of the GAN loss. Which may explain for the difficulty of training. (See [[this|http://www.inference.vc/infogan-variational-bound-on-mutual-information-twice/]])

And in the ''least-squares importance estimation'' case, we take loss:
$$
\mathcal L(s;P,Q)=\mathbb E_{x\sim P}s^2(x)-2\mathbb E_{x\sim Q}s(x),
$$
and the Bayes-optimal discrepancy function becomes
$$
s^*(x) = \frac{Q(s(x))}{P(x)}
$$
the same as logistic regression but without the logarithm, and this turns out to be a very important difference. For the very simple and intuitive derivation , see [[Kanamori et al, 2009|http://www.jmlr.org/papers/volume10/kanamori09a/kanamori09a.pdf]].

This is similar to the definition of the Rényi $$\alpha$$-divergence I wrote about last week, except for a missing logarithm. If you choose a nonlinearity $$(x)=x^{\alpha-2}$$, you can recover Rényi divergences for different alpha.

! Gradient Estimators
* [[Gradient Estimators for Implicit Models|https://arxiv.org/abs/1705.07107]]