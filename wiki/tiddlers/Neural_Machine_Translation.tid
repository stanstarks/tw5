created: 20170614091659389
modified: 20171231045304399
tags: NLP
title: Neural Machine Translation
type: text/vnd.tiddlywiki

! Bibs
* [[Convolutional Sequence to Sequence Learning]]
* [[Attention Is All You Need]]
* [[Non-Autoregressive Neural Machine Translation]]

! Architectures

''Kalchbrenner and Blunsom 2013''  CSM Encoder, decode with RNN, predict with a linear softmax output.

* Convolutions learn interactions among features in a local context
* By stacking them, longer range dependencies can be learnt
* Deep ConvNets have a branching structure similar to trees, but not parser is required
* To handle variable length, see [[A Convolutional Neural Network for Modelling Sentences|https://arxiv.org/abs/1404.2188]]

''Sutskever et al. 2014'' seq2seq. The hidden state has to remember a lot of information. Following tricks improve performanceï¼š

* backwards input
* decoder ensemble

! Improvements
* [[Attention Mechanism]] add attention to seq2seq translation: +11 [[BLEU]]

! Toolkit
* [[OpenNMT|https://github.com/opennmt/opennmt]]
* [[Nematus|https://github.com/EdinburghNLP/nematus]]: Attention-based encoder-decoder model from EdinburghNLP