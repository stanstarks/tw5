created: 20191121033824189
modified: 20191122051249155
tags: Meta-Learning
title: MAML Interpretation
type: text/vnd.tiddlywiki

[[link|https://www.inference.vc/notes-on-imaml-meta-learning-without-differentiating-through/]]

In meta-learning, we have a series of independent tasks, with associated training and validation loss functions $$f_i$$ and $$g_i$$, respectively. We have a set of model parameters $$\theta$$ which are shared across the tasks, and the loss functions $$f_i(\theta)$$ and $$g_i(\theta)$$ evaluate how well the model with parameters $$\theta$$ does on the training and test cases of task $$i$$. We have an algorithm that has access to the training loss fi and some meta-parameters $$\theta_0$$, and output some optimal or learned parameters $$\theta^*_i=\operatorname{Alg}(f_i,\theta_0)$$. The goal of the meta-learning algorithm is to optimize the meta-objective
$$
\mathcal{M}(\theta_0) = \sum_i g_i(\operatorname{Alg}(f_i, \theta_0))
$$
with respect to the meta-parameters $$\theta_0$$.

In the early version of [[MAML|Model-Agnostic Meta-Learning]], the algorithm was chosen to be SGD, $$f_i$$ and $$g_i$$ being the training and test loss.

! Stochasticity
A more generous formulation of the meta-objective would allow for stochastic algorithms. If we denote by $$\operatorname{Alg}(f_i, \theta_0)$$ the distribution over solutions the algorithm finds, the meta-objective would be
$$
\mathcal{M}_{stochastic}(\theta_0) = \sum_i \mathbb E_{\theta\sim\operatorname{Alg}(f_i, \theta_0)}g_i(\theta)
$$

The meta-learning objective now depends on $$\theta_0$$ in two ways:

* Differentiable
* The probability with which we find the different solutions change.

! Bayesian
One can think of the anchor point $$\theta_0$$ as the mean of a prior distribution over the neural networks's weights. The inner loop of the algorithm, or $$\operatorname{Alg}(f_i,\theta_0)$$ then finds the maximum-a-posteriori (MAP) approximation to the posterior over $$\theta$$.

In Baysian, we seek to optimize $$\theta_0$$ by maximising the marginal likelihood:
$$
\mathcal{M}_{\text{variational}}(\theta_0, Q_i) = \sum_i \left( KL[Q_i\vert \mathcal{N}_{\theta_0}] + \mathbb{E}_{\theta \sim Q_i} f_i(\theta) \right),
$$
where $$Q_i$$ approximates the posterior over model parameters for task $$i$$. A specific choice of $$Q_i$$ is a dirac delta distribution centred at a specific point $$Q_i(\theta) = \delta(\theta - \theta^{\ast}_i)$$. If we generously ignore some constants that blow up to infinitely large, the KL divergence between the Gaussian prior and the degenerate point-posterior is a simple Euclidean distance, and our variational objective reduces to:
$$
\mathcal{M}_{\text{variational}}(\theta_0, \theta_i) = \sum_i \left( \|\theta_i - \theta_0\|^2 + f_i(\theta_i) \right)
$$

Similar idea:

* [[Meta-Learning Probabilistic Inference For Prediction|https://arxiv.org/abs/1805.09921]]